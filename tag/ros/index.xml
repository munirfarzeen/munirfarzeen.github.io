<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ROS | Farzeen Munir</title>
    <link>https://munirfarzeen.github.io./tag/ros/</link>
      <atom:link href="https://munirfarzeen.github.io./tag/ros/index.xml" rel="self" type="application/rss+xml" />
    <description>ROS</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 27 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://munirfarzeen.github.io./media/icon_hu71df4acb26db2b7b409c33d00b1abd17_7999_512x512_fill_lanczos_center_3.png</url>
      <title>ROS</title>
      <link>https://munirfarzeen.github.io./tag/ros/</link>
    </image>
    
    <item>
      <title>BeetleBot</title>
      <link>https://munirfarzeen.github.io./project/beetlebot/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://munirfarzeen.github.io./project/beetlebot/</guid>
      <description>&lt;p&gt;The recent development in the field of mobile robotics has made them available for commercial and research  purposes.  The  primary  challenges  that  are  encountered  by  deploying  the  mobile  robot  in  a dynamic environment is mapping and navigation. Simultaneous localization and mapping (SLAM) provide a good understanding of the environment for navigation and path planning. In this work, we explore the problem  of  mapping  and  navigation  by  incorporating  the  semantics  of  the  environment.  For  the experimental  setup,  a  robot  (BeetleBot)  is  designed  having  equipped  with  Kobuki  mobile  base, Realsense  RGB-D  camera,  range  sensors  and  NVidia  Jetson  Xavier  as  computation  computer.  The autonomous semantic mapping and navigation are performed using RTAB-MAP with the inclusion of A* algorithm for exploring and updating the unknown environment and deep learning-based object detection algorithm. A Proportional-Integral-Derivative (PID) is implemented as a controller for the BeetleBOT. We have used the Robot Operating System (ROS) as a software development platform for the BeetleBOT. The experimental evaluation shows the mapping and localization efficacy using the BeetleBOT as our mobile robot.&lt;/p&gt;
















&lt;figure  id=&#34;figure-overall-architecture-of-beetlebot-using-ros-framework&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Overall architecture of BeetleBot using ROS framework.&#34; srcset=&#34;
               /media/turtlebot_hufa6ccf348658e8a4b78494a96f18ce67_159165_9f640d87928ee35e1cc9e383f449ae18.webp 400w,
               /media/turtlebot_hufa6ccf348658e8a4b78494a96f18ce67_159165_f219b8a0b0981f025879c39ffb411c6f.webp 760w,
               /media/turtlebot_hufa6ccf348658e8a4b78494a96f18ce67_159165_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://munirfarzeen.github.io./media/turtlebot_hufa6ccf348658e8a4b78494a96f18ce67_159165_9f640d87928ee35e1cc9e383f449ae18.webp&#34;
               width=&#34;760&#34;
               height=&#34;369&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Overall architecture of BeetleBot using ROS framework.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The demonstration video of the project.&lt;/p&gt;
&lt;p&gt;








  





&lt;video controls  &gt;
  &lt;source src=&#34;https://munirfarzeen.github.io./media/robot%20navigation%202.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://munirfarzeen.github.io./media/video_2021-11-01_01-05-52.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
